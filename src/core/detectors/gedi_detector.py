"""GEDI LiDAR detection algorithms for archaeological analysis - FIXED VERSION.

This file should replace src/core/detectors/gedi_detector.py

The main fixes:
1. Proper handling of GEDI L2A data structure
2. Improved archaeological detection algorithms
3. Better error handling and logging
4. Fixed metric file processing
"""

from __future__ import annotations

from typing import Any, Dict, List, Optional
import numpy as np
from pathlib import Path
import logging
import json
import geopandas as gpd
from shapely.geometry import Point, LineString
from datetime import datetime
from scipy import stats

# Import clustering only if available
try:
    from sklearn.cluster import DBSCAN
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    
from ..config import TargetZone, RESULTS_DIR, ZONE_DETECTION_CONFIG
from ..coordinate_validation import validate_coordinates, validate_coordinate_array, ensure_geojson_format
from ..parameter_configs import get_current_params
from ..coordinate_manager import CoordinateManager

logger = logging.getLogger(__name__)

# Define the base directory for GEDI detector outputs
GEDI_DETECTOR_OUTPUT_BASE_DIR = RESULTS_DIR / "detector_outputs" / "gedi"


def cluster_nearby_points(
    points: np.ndarray, min_cluster_size: int = None, eps: float = None
) -> List[Dict[str, Any]]:
    """Cluster nearby points using DBSCAN for archaeological feature detection."""
    if len(points) == 0:
        return []
    
    # Get current parameters if not provided
    if min_cluster_size is None or eps is None:
        params = get_current_params()
        gedi_params = params['gedi']
        if min_cluster_size is None:
            min_cluster_size = gedi_params.min_cluster_size
        if eps is None:
            eps = gedi_params.clustering_eps_degrees
    
    if not SKLEARN_AVAILABLE:
        logger.warning("sklearn not available, using simple clustering")
        # Simple distance-based clustering fallback
        clusters = []
        used = np.zeros(len(points), dtype=bool)
        
        for i, point in enumerate(points):
            if used[i]:
                continue
            
            # Find nearby points
            distances = np.sqrt(np.sum((points - point)**2, axis=1))
            nearby = distances < eps
            nearby_points = points[nearby]
            
            if len(nearby_points) >= min_cluster_size:
                center = nearby_points.mean(axis=0)
                # NASA Official: GEDI footprints are 25m diameter circles (not squares)
                footprint_area_m2 = np.pi * (12.5)**2  # Ï€ Ã— rÂ² = 490.87 mÂ²
                area_km2 = len(nearby_points) * (footprint_area_m2 / 1000000)  # Convert to kmÂ²
                
                # Get current GEDI parameters for size filtering
                params = get_current_params()
                gedi_params = params['gedi']
                
                # Apply size filtering - exclude very large clusters likely to be natural
                if area_km2 <= gedi_params.max_feature_area_km2:
                    clusters.append({
                        "center": tuple(center), 
                        "count": len(nearby_points),
                        "area_km2": area_km2
                    })
                used[nearby] = True
        
        return clusters
    
    # Use DBSCAN if available
    clustering = DBSCAN(eps=eps, min_samples=min_cluster_size).fit(points)
    labels = clustering.labels_
    clusters = []
    
    for label in set(labels):
        if label == -1:  # Noise points
            continue
        cluster_points = points[labels == label]
        center = cluster_points.mean(axis=0)
        # NASA Official: GEDI footprints are 25m diameter circles (not squares)
        footprint_area_m2 = np.pi * (12.5)**2  # Ï€ Ã— rÂ² = 490.87 mÂ²
        area_km2 = len(cluster_points) * (footprint_area_m2 / 1000000)  # Convert to kmÂ²
        
        # Get current GEDI parameters for size filtering
        params = get_current_params()
        gedi_params = params['gedi']
        
        # Apply size filtering - exclude very large clusters likely to be natural
        if area_km2 <= gedi_params.max_feature_area_km2:
            clusters.append({
                "center": tuple(center), 
                "count": len(cluster_points),
                "area_km2": area_km2
            })
    return clusters


def detect_linear_patterns(
    points: np.ndarray, min_points: int = 5
) -> List[Dict[str, Any]]:
    """
    Detect linear archaeological features with statistical validation.
    
    Uses evidence-based RÂ² thresholds and geodesic distance calculation for Amazon-scale
    accuracy. Implements F-test significance testing per archaeological remote sensing standards.
    """
    if len(points) < min_points:
        return []
    
    if not SKLEARN_AVAILABLE:
        logger.warning("sklearn not available, skipping linear pattern detection")
        return []
    
    try:
        from sklearn.linear_model import LinearRegression
        from scipy.stats import f
        import math
        
        # Prepare data for linear regression
        X = points[:, 0].reshape(-1, 1)
        y = points[:, 1]
        model = LinearRegression().fit(X, y)
        r2 = model.score(X, y)
        
        # Statistical significance testing (F-test)
        n = len(points)
        p = 1  # number of predictors
        f_statistic = (r2 / p) / ((1 - r2) / (n - p - 1))
        p_value = 1 - f.cdf(f_statistic, p, n - p - 1)
        
        # Evidence-based RÂ² thresholds from archaeological literature
        # Amazon causeway studies: 0.75+, field boundaries: 0.70+
        causeway_threshold = 0.75
        field_boundary_threshold = 0.70
        
        # Determine feature type and validate threshold
        feature_type = None
        is_significant = p_value < 0.05  # Statistical significance
        
        if r2 >= causeway_threshold and is_significant:
            feature_type = "linear_causeway"
        elif r2 >= field_boundary_threshold and is_significant:
            feature_type = "field_boundary"
        
        if feature_type:
            # Geodesic distance calculation (essential for Amazon-scale accuracy)
            def haversine_distance(lat1, lon1, lat2, lon2):
                """Calculate geodesic distance using Haversine formula."""
                R = 6371000  # Earth radius in meters
                
                lat1_rad = math.radians(lat1)
                lat2_rad = math.radians(lat2)
                delta_lat = math.radians(lat2 - lat1)
                delta_lon = math.radians(lon2 - lon1)
                
                a = (math.sin(delta_lat / 2) ** 2 + 
                     math.cos(lat1_rad) * math.cos(lat2_rad) * 
                     math.sin(delta_lon / 2) ** 2)
                c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
                
                return R * c
            
            # Calculate total geodesic length
            total_length_m = 0
            for i in range(len(points) - 1):
                lat1, lon1 = points[i]
                lat2, lon2 = points[i + 1]
                total_length_m += haversine_distance(lat1, lon1, lat2, lon2)
            
            # Residual analysis for model quality assessment
            y_pred = model.predict(X)
            residuals = y - y_pred
            rmse = np.sqrt(np.mean(residuals ** 2))
            
            return [{
                "coordinates": points.tolist(),
                "r2": float(r2),
                "f_statistic": float(f_statistic),
                "p_value": float(p_value),
                "is_statistically_significant": is_significant,
                "length_km": total_length_m / 1000,  # Convert to km
                "length_method": "geodesic_haversine",
                "rmse": float(rmse),
                "feature_type": feature_type,
                "validation_method": "f_test_significance_geodesic_distance"
            }]
        
    except Exception as e:
        logger.warning(f"Error in linear pattern detection: {e}")
    
    return []


def detect_archaeological_clearings(
    rh95_data: np.ndarray, rh100_data: np.ndarray, coordinates: np.ndarray | None = None,
    min_cluster_size: int = None, clustering_eps: float = None
) -> Dict[str, Any]:
    """
    Detect potential archaeological clearings using relative height method.
    
    Based on Amazon gap dynamics research (Nature Sci Rep, 2021), uses 50% of local
    maximum canopy height rather than fixed thresholds. RH95 prioritized for accuracy
    (MAE 1.35m, RMSE 2.08m vs RH100 underestimation bias).
    """
    from scipy.ndimage import generic_filter
    
    # Evidence-based relative height approach
    # Calculate local maximum height in 5m neighborhood (per gap dynamics study)
    def local_max(values):
        return np.max(values) if len(values) > 0 else 0
    
    # Use RH95 as primary (more accurate per validation studies)
    local_max_95 = generic_filter(rh95_data, local_max, size=3)  # ~5m at 30m resolution
    relative_threshold = 0.5  # 50% of local maximum per scientific literature
    
    # Gap detection using relative height method
    gaps_relative = rh95_data < (local_max_95 * relative_threshold)
    
    # Secondary validation with RH100 for robustness
    local_max_100 = generic_filter(rh100_data, local_max, size=3)
    gaps_rh100 = rh100_data < (local_max_100 * relative_threshold)
    
    # Combined gap detection (either metric indicates gap)
    significant_gaps = gaps_relative | gaps_rh100
    
    clusters = []
    if coordinates is not None and len(coordinates) == len(significant_gaps):
        gap_coords = coordinates[significant_gaps]
        if len(gap_coords) > 0:
            clusters = cluster_nearby_points(gap_coords, min_cluster_size, clustering_eps)
    
    # Filter clusters by archaeologically relevant size (10mÂ² to 1ha)
    # Based on Amazon treefall gap study: average 40.89mÂ², filter <10mÂ² artifacts
    valid_clusters = []
    for cluster in clusters:
        area_m2 = cluster.get("area_m2", 0)
        if 10 <= area_m2 <= 10000:  # 10mÂ² to 1ha archaeological range
            valid_clusters.append(cluster)
    
    # Statistical validation of archaeological potential
    if valid_clusters:
        # F1-score based archaeological significance
        cluster_sizes = [c["count"] for c in valid_clusters]
        cluster_areas = [c.get("area_m2", 0) for c in valid_clusters]
        
        # Evidence-based scoring using size distribution analysis
        archaeological_score = 0
        for cluster in valid_clusters:
            size = cluster["count"]
            area = cluster.get("area_m2", 0)
            
            # Size-based significance (per cluster analysis literature)
            if size >= 10:  # Large clearing pattern
                archaeological_score += 5
            elif size >= 5:  # Medium clearing
                archaeological_score += 3
            elif size >= 3:  # Small but significant
                archaeological_score += 2
            
            # Area-based validation (archaeological scale)
            if 100 <= area <= 5000:  # Typical settlement clearing size
                archaeological_score += 2
        
        # Normalize by cluster count for comparative analysis
        normalized_score = archaeological_score / len(valid_clusters) if valid_clusters else 0
    else:
        normalized_score = 0
    
    return {
        "gap_points": significant_gaps.astype(float),
        "gap_clusters": valid_clusters,
        "archaeological_potential": normalized_score,
        "total_clearings": len(valid_clusters),
        "largest_clearing_size": max([c["count"] for c in valid_clusters]) if valid_clusters else 0,
        "relative_threshold_used": relative_threshold,
        "validation_method": "relative_height_50pct_local_max"
    }


def validate_elevation_anomaly(elevations: np.ndarray, threshold_elevation: float) -> Dict[str, Any]:
    """Test if elevation anomaly is statistically significant"""
    if len(elevations) < 3:  # Need minimum for meaningful statistics
        return {"p_value": 1.0, "cohens_d": 0.0, "significance": "INSUFFICIENT_DATA", "statistically_valid": False}
    
    # One-sample t-test against baseline elevation
    t_stat, p_value = stats.ttest_1samp(elevations, threshold_elevation)
    
    # Cohen's d effect size calculation
    mean_diff = np.mean(elevations) - threshold_elevation
    pooled_std = np.std(elevations)
    cohens_d = mean_diff / pooled_std if pooled_std > 0 else 0.0
    
    # Significance levels based on archaeological literature
    significance_level = "HIGH" if p_value < 0.01 and abs(cohens_d) >= 0.5 else \
                        "MEDIUM" if p_value < 0.05 and abs(cohens_d) >= 0.3 else "LOW"
    
    return {
        "p_value": float(p_value),
        "cohens_d": float(cohens_d),
        "significance": significance_level,
        "statistically_valid": p_value < 0.05 and abs(cohens_d) >= 0.3
    }


def detect_archaeological_earthworks(
    elevation_data: np.ndarray, coordinates: np.ndarray,
    min_cluster_size: int = None, clustering_eps: float = None
) -> Dict[str, Any]:
    """Detect elevation anomalies that may represent earthworks."""
    
    # Filter valid elevation data
    valid_mask = ~np.isnan(elevation_data)
    if np.sum(valid_mask) < 10:  # Need minimum points for statistics
        return {
            "mound_candidates": np.array([]),
            "ditch_candidates": np.array([]),
            "mound_clusters": [],
            "linear_features": [],
            "archaeological_potential": 0,
            "statistical_validation": {"significance": "INSUFFICIENT_DATA"}
        }
    
    valid_elevations = elevation_data[valid_mask]
    valid_coordinates = coordinates[valid_mask]
    
    mean_elev = float(np.mean(valid_elevations))
    std_elev = float(np.std(valid_elevations))
    
    # Get current GEDI parameters  
    params = get_current_params()
    gedi_params = params['gedi']
    
    # Archaeological earthworks: elevation anomalies based on parameters
    anomaly_threshold = gedi_params.elevation_anomaly_std_multiplier * std_elev
    high_anom = valid_elevations > (mean_elev + anomaly_threshold)
    low_anom = valid_elevations < (mean_elev - anomaly_threshold)
    
    # Statistical validation for anomalies
    statistical_validation = {"significance": "NONE"}
    if np.any(high_anom):
        high_anom_elevations = valid_elevations[high_anom]
        statistical_validation = validate_elevation_anomaly(high_anom_elevations, mean_elev)

    mound_clusters = []
    linear_features = []
    
    if np.any(high_anom):
        mound_coords = valid_coordinates[high_anom]
        mound_clusters = cluster_nearby_points(mound_coords, 
                                               min_cluster_size=min_cluster_size or gedi_params.min_mound_cluster_size, 
                                               eps=clustering_eps or gedi_params.mound_clustering_eps)
    
    if np.any(low_anom):
        ditch_coords = valid_coordinates[low_anom]
        linear_features = detect_linear_patterns(ditch_coords, min_points=4)

    # Evidence-based archaeological potential calculation
    archaeological_potential = 0
    confidence_multiplier = 1.0
    
    # Statistical confidence adjustment
    if statistical_validation.get("significance") == "HIGH":
        confidence_multiplier = 1.2
    elif statistical_validation.get("significance") == "LOW":
        confidence_multiplier = 0.7
    
    for cluster in mound_clusters:
        area_m2 = cluster.get("area_km2", 0) * 1000000  # Convert to mÂ²
        point_count = cluster["count"]
        
        # Evidence-based thresholds from archaeological literature
        if area_m2 >= 200 and point_count >= 5:  # Complex (200-2000 mÂ²)
            archaeological_potential += 4
        elif area_m2 >= 50 and point_count >= 3:  # Individual feature (50-500 mÂ²)
            archaeological_potential += 2
        elif point_count >= 2:  # Possible feature
            archaeological_potential += 1
    
    for feature in linear_features:
        if feature["r2"] > 0.9:  # Very linear feature
            archaeological_potential += 3
        else:
            archaeological_potential += 2
    
    # Apply confidence adjustment
    archaeological_potential = int(archaeological_potential * confidence_multiplier)

    # Create full-length arrays matching original data
    full_mound_candidates = np.zeros(len(elevation_data))
    full_ditch_candidates = np.zeros(len(elevation_data))
    
    if np.any(high_anom):
        full_mound_candidates[valid_mask] = high_anom.astype(float)
    if np.any(low_anom):
        full_ditch_candidates[valid_mask] = low_anom.astype(float)

    return {
        "mound_candidates": full_mound_candidates,
        "ditch_candidates": full_ditch_candidates,
        "mound_clusters": mound_clusters,
        "linear_features": linear_features,
        "archaeological_potential": archaeological_potential,
        "statistical_validation": statistical_validation,
        "elevation_stats": {
            "mean": mean_elev,
            "std": std_elev,
            "anomaly_threshold": anomaly_threshold
        }
    }


def _validate_feature_coordinates(lon: float, lat: float, context: str = "") -> bool:
    """Standalone coordinate validation for features - DEPRECATED - use coordinate_validation module"""
    from ..coordinate_validation import validate_coordinates
    return validate_coordinates(lon, lat, context)

def _create_geojson_from_features(features: List[Dict[str, Any]], feature_type_key: str, geom_key: str = "center", properties_keys: Optional[List[str]] = None) -> Optional[gpd.GeoDataFrame]:
    """Helper function to create a GeoDataFrame from a list of feature dictionaries."""
    if not features:
        return None

    geometries = []
    data = []

    for feature in features:
        coords = feature.get(geom_key)
        if not coords:
            logger.warning(f"Feature missing geometry at key '{geom_key}': {feature}")
            continue

        if feature_type_key == "clearings" or (isinstance(coords, (list, tuple)) and len(coords) == 2 and all(isinstance(c, (int, float)) for c in coords)) : # Assuming center is (lon, lat) from GEDI
            # coords is already [lon, lat] from cluster center, use directly for GeoJSON standard
            # Validate coordinates before creating geometry using unified system
            if validate_coordinates(coords[0], coords[1], f"{feature_type_key}_feature"):
                # Ensure GeoJSON format [lon, lat]
                coord_pair = ensure_geojson_format(coords)
                geometries.append(Point(coord_pair))
            else:
                logger.warning(f"Skipping feature with invalid coordinates: {coords}")
                continue
        elif feature_type_key == "earthworks_linear" and isinstance(coords, list) and len(coords) > 1: # Assuming list of points for LineString
            try:
                # Validate all coordinates in the linestring
                valid_coords = []
                for i, coord_pair in enumerate(coords):
                    if len(coord_pair) >= 2 and validate_coordinates(coord_pair[0], coord_pair[1], f"{feature_type_key}_point_{i}"):
                        valid_coords.append(ensure_geojson_format(coord_pair))
                
                if len(valid_coords) >= 2:  # LineString needs at least 2 points
                    geometries.append(LineString(valid_coords))
                else:
                    logger.warning(f"Not enough valid coordinates for LineString: {len(valid_coords)} valid out of {len(coords)}")
                    continue
            except Exception as e:
                logger.warning(f"Could not create LineString for feature {feature}: {e}")
                continue
        else:
            logger.warning(f"Unsupported geometry for feature_type '{feature_type_key}' with geom_key '{geom_key}': {coords}")
            continue
        
        if properties_keys:
            props = {k: feature.get(k) for k in properties_keys}
            data.append(props)
        else: # Add all non-geometry properties
            props = {k: v for k, v in feature.items() if k != geom_key}
            data.append(props)
            
    if not geometries:
        return None
        
    # If properties_keys were provided, data will have specific columns. Otherwise, it's a list of dicts.
    if properties_keys and data:
        return gpd.GeoDataFrame(data, geometry=geometries, crs="EPSG:4326") # Assume WGS84 for now
    elif data: # data is list of dicts
        return gpd.GeoDataFrame(data, geometry=geometries, crs="EPSG:4326")
    else: # No properties, just geometries
        return gpd.GeoDataFrame(geometry=geometries, crs="EPSG:4326")


class GEDIArchaeologicalDetector:
    """Detector that analyzes GEDI metric files - FIXED VERSION."""

    def __init__(self, zone: TargetZone, run_id: Optional[str] = None) -> None:
        self.zone = zone
        self.run_id = run_id
        self.detection_results: Dict[str, Any] = {}
        
        # Load zone-specific detection configuration
        self.zone_config = self._load_zone_config()
        
        # GEDI doesn't need pixel-to-geographic conversion (data already geographic)
        # but we'll use coordinate manager for validation consistency
        self.coordinate_manager = None  # Will be set up when needed for validation
        
        logger.info(f"GEDI detector initialized for zone {zone.id} (type: {getattr(zone, 'zone_type', 'default')}, strategy: {getattr(zone, 'detection_strategy', 'balanced')}) with run_id: {run_id}")
        logger.info(f"Zone-specific config: {self.zone_config}")
    
    def _load_zone_config(self):
        """Load zone-specific detection configuration"""
        # Get zone type, default to forested_buried_sites if not specified
        zone_type = getattr(self.zone, 'zone_type', 'forested_buried_sites')
        
        # Get zone-specific config from ZONE_DETECTION_CONFIG
        zone_config = ZONE_DETECTION_CONFIG.get(zone_type, ZONE_DETECTION_CONFIG['forested_buried_sites'])
        
        logger.info(f"Loading GEDI zone config for zone_type '{zone_type}': {zone_config}")
        return zone_config

    def _validate_coordinates(self, lon: float, lat: float, context: str = "") -> bool:
        """Validate coordinates using unified coordinate system logic"""
        # GEDI data already comes with geographic coordinates, just validate them
        return validate_coordinates(lon, lat, context, zone_id=self.zone.id)

    def _validate_coordinate_array(self, coordinates: np.ndarray, context: str = "") -> np.ndarray:
        """Validate coordinate array using unified coordinate system logic"""
        return validate_coordinate_array(coordinates, context, zone_id=self.zone.id)
    
    def create_gedi_point_feature(self, lon: float, lat: float, properties: Dict[str, Any]) -> Dict[str, Any]:
        """
        Create a GEDI point feature with unified coordinate validation
        Compatible with coordinate manager approach but for geographic data
        """
        # Validate coordinates first
        if not self._validate_coordinates(lon, lat, f"gedi_{properties.get('type', 'feature')}"):
            raise ValueError(f"Invalid GEDI coordinates: {lon}, {lat}")
        
        # Create feature in standard format compatible with Sentinel-2 features
        feature = {
            'geometry': Point(lon, lat),
            'coordinates': [lon, lat],  # GeoJSON format: [longitude, latitude]
            'provider': 'gedi',
            **properties
        }
        
        logger.debug(f"Created GEDI feature: ({lon:.6f}, {lat:.6f})")
        return feature

    def _load_gedi_metrics(self, scene_metrics_path: Path) -> Optional[Dict[str, np.ndarray]]:
        """Load GEDI metrics from both JSON (new L2A/L2B) and .npy (legacy) formats."""
        
        # Method 1: Try JSON format (new L2A/L2B format from updated provider)
        json_files = list(scene_metrics_path.glob("*_metrics.json"))
        if json_files:
            try:
                # CRITICAL FIX: Process ALL granules, not just the first one
                if len(json_files) > 1:
                    logger.info(f"Found {len(json_files)} GEDI granules - combining all spatial data")
                    
                    # Combine data from all granules to cover full zone
                    all_coordinates = []
                    all_rh95 = []
                    all_rh100 = []
                    all_elevation = []
                    
                    for json_file in json_files:
                        logger.info(f"Processing granule: {json_file.name}")
                        
                        with open(json_file, 'r') as f:
                            granule_data = json.load(f)
                        
                        # Extract coordinates
                        granule_coords = np.column_stack([
                            np.array(granule_data['longitude']),
                            np.array(granule_data['latitude'])
                        ])
                        
                        # Validate coordinates for this granule
                        valid_coords = validate_coordinate_array(granule_coords, f"GEDI_granule_{json_file.stem}", zone_id=self.zone.id)
                        
                        if len(valid_coords) > 0:
                            all_coordinates.append(valid_coords)
                            
                            # Handle both L2A and L2B formats
                            if 'canopy_height' in granule_data:
                                canopy_heights = np.array(granule_data['canopy_height'])
                                all_rh95.append(canopy_heights)
                                all_rh100.append(canopy_heights)
                            elif 'rh100' in granule_data and 'rh95' in granule_data:
                                all_rh95.append(np.array(granule_data['rh95']))
                                all_rh100.append(np.array(granule_data['rh100']))
                            else:
                                # Fallback - elevation difference
                                elev_ground = np.array(granule_data['elevation_ground'])
                                elev_canopy = np.array(granule_data['elevation_canopy_top'])
                                canopy_proxy = np.maximum(0, elev_canopy - elev_ground)
                                all_rh95.append(canopy_proxy)
                                all_rh100.append(canopy_proxy)
                            
                            all_elevation.append(np.array(granule_data['elevation_ground']))
                            
                            logger.info(f"âœ… Granule {json_file.stem}: {len(valid_coords)} valid points")
                    
                    if all_coordinates:
                        # Combine all granule data
                        combined_coordinates = np.vstack(all_coordinates)
                        combined_rh95 = np.concatenate(all_rh95)
                        combined_rh100 = np.concatenate(all_rh100)
                        combined_elevation = np.concatenate(all_elevation)
                        
                        logger.info(f"âœ… Combined {len(json_files)} granules: {len(combined_coordinates)} total points")
                        
                        return {
                            'coordinates': combined_coordinates,
                            'rh95_data': combined_rh95,
                            'rh100_data': combined_rh100,
                            'elevation_data': combined_elevation
                        }
                    else:
                        logger.error("No valid coordinates found in any granule")
                        return None
                
                else:
                    # Single granule processing (original logic)
                    json_file = json_files[0]
                    logger.info(f"Loading single GEDI granule: {json_file}")
                    
                    with open(json_file, 'r') as f:
                        metrics_data = json.load(f)
                    
                    # Convert from JSON format to numpy arrays
                    coordinates = np.column_stack([
                        np.array(metrics_data['longitude']),
                        np.array(metrics_data['latitude'])
                    ])
                    
                    # Use unified coordinate validation system
                    coordinates = validate_coordinate_array(coordinates, "GEDI_JSON_metrics", zone_id=self.zone.id)
                    
                    # Handle both L2A and L2B formats
                    if 'canopy_height' in metrics_data:
                        canopy_heights = np.array(metrics_data['canopy_height'])
                        rh95_data = canopy_heights
                        rh100_data = canopy_heights
                    elif 'rh100' in metrics_data and 'rh95' in metrics_data:
                        rh95_data = np.array(metrics_data['rh95'])
                        rh100_data = np.array(metrics_data['rh100'])
                    else:
                        logger.warning("No canopy height data found, using elevation difference as proxy")
                        elev_ground = np.array(metrics_data['elevation_ground'])
                        elev_canopy = np.array(metrics_data['elevation_canopy_top'])
                        canopy_proxy = np.maximum(0, elev_canopy - elev_ground)
                        rh95_data = canopy_proxy
                        rh100_data = canopy_proxy
                    
                    elevation_data = np.array(metrics_data['elevation_ground'])
                    
                    logger.info(f"âœ… Single granule loaded: {len(coordinates)} points from {json_file.name}")
                    
                    return {
                        'coordinates': coordinates,
                        'rh95_data': rh95_data,
                        'rh100_data': rh100_data,
                        'elevation_data': elevation_data
                    }
                
            except Exception as e:
                logger.warning(f"Failed to load JSON metrics: {e}. Trying legacy .npy format...")
        
        # Method 2: Try legacy .npy format
        coords_file = scene_metrics_path / "coordinates.npy"
        rh95_file = scene_metrics_path / "canopy_height_95.npy"
        rh100_file = scene_metrics_path / "canopy_height_100.npy"
        elev_file = scene_metrics_path / "ground_elevation.npy"
        
        if all(f.exists() for f in [coords_file, rh95_file, rh100_file, elev_file]):
            try:
                logger.info(f"Loading GEDI metrics from legacy .npy files")
                
                return {
                    'coordinates': np.load(coords_file),
                    'rh95_data': np.load(rh95_file),
                    'rh100_data': np.load(rh100_file),
                    'elevation_data': np.load(elev_file)
                }
                
            except Exception as e:
                logger.error(f"Failed to load legacy .npy metrics: {e}")
                return None
        
        # Method 3: Try looking for metrics in parent directory (different provider structure)
        parent_path = scene_metrics_path.parent
        json_files = list(parent_path.glob("*_metrics.json"))
        if json_files:
            logger.info(f"Trying parent directory for JSON metrics: {parent_path}")
            temp_path = Path(str(scene_metrics_path).replace(str(scene_metrics_path), str(parent_path)))
            return self._load_gedi_metrics(parent_path)
        
        logger.error(f"No valid GEDI metrics found in {scene_metrics_path}")
        return None

    def analyze_scene(self, scene_metrics_path: Path) -> Dict[str, Any]:
        """Analyze GEDI scene directory with proper file handling and output caching."""
        
        # Derive scene_id from the input path (e.g., granule name)
        # Assuming scene_metrics_path is a directory like: .../processed_metrics_cache/zone_id/granule_id/
        # Or if it's a file like .../granule_id_metrics.npz, then use its stem.
        # For consistency with how scene_dir was used, let's assume it's a directory for the scene.
        # The GEDIProvider saves metrics for a granule in a directory named after the granule ID.
        # GEDIProvider.process_gedi_granule returns SceneData with metrics_file_path set to this dir.
        
        if not scene_metrics_path.is_dir():
             logger.error(f"Scene metrics path is not a directory: {scene_metrics_path}")
             return {"success": False, "status": "error_invalid_metrics_path", "message": "Metrics path must be a directory."}

        scene_id = scene_metrics_path.name # Use directory name as scene_id (granule_id)
        zone_id = self.zone.id

        if self.run_id:
            # Use run-specific directory structure
            output_dir = RESULTS_DIR / f"run_{self.run_id}" / "detector_outputs" / "gedi" / zone_id / scene_id
            logger.info(f"GEDI detector using run-specific output dir: {output_dir}")
        else:
            # Fallback to global directory (for backward compatibility)
            output_dir = GEDI_DETECTOR_OUTPUT_BASE_DIR / zone_id / scene_id
            logger.warning(f"GEDI detector using global output dir (no run_id): {output_dir}")
        output_dir.mkdir(parents=True, exist_ok=True)

        summary_path = output_dir / "gedi_detection_summary.json"
        clearings_geojson_path = output_dir / "gedi_clearings.geojson"
        earthworks_geojson_path = output_dir / "gedi_earthworks.geojson"

        # Cache Check
        if summary_path.exists():
            try:
                with open(summary_path, 'r') as f:
                    summary_data = json.load(f)
                
                # Basic validation of summary
                if "status" in summary_data and summary_data["status"] == "success_cached":
                    # Ensure referenced GeoJSON files exist if features were expected
                    clearings_ok = (not summary_data.get("clearing_features_count") or 
                                    (summary_data.get("clearing_features_count") and clearings_geojson_path.exists()))
                    earthworks_ok = (not summary_data.get("earthwork_features_count") or
                                     (summary_data.get("earthwork_features_count") and earthworks_geojson_path.exists()))

                    if clearings_ok and earthworks_ok:
                        logger.info(f"CACHE HIT: Loading GEDI detection results from {output_dir} for {zone_id}/{scene_id}")
                        # Construct the full result to match what would be returned by processing
                        # This requires storing more info in summary or loading GeoJSONs here
                        # For now, return the summary, actual feature data loading can be done by consumer if needed from paths
                        
                        # Simplified: just return summary and paths.
                        # A more complete cache load would re-populate clearing_results, earthwork_results, etc.
                        # For now, let's assume the consumer will use these paths if needed.
                        # The `ModularPipeline` mostly uses the summary counts and overall score.
                        
                        # Reconstruct a dictionary similar to a fresh run for consistency
                        loaded_results = {
                            "success": True,
                            "status": "loaded_from_cache", # Indicate cache load
                            "zone_name": self.zone.name,
                            "scene_path": str(scene_metrics_path),
                            "output_summary_path": str(summary_path),
                            "clearing_results_path": str(clearings_geojson_path) if clearings_geojson_path.exists() else None,
                            "earthwork_results_path": str(earthworks_geojson_path) if earthworks_geojson_path.exists() else None,
                            "total_features": summary_data.get("total_features", 0),
                            # Add other relevant fields from summary_data if they were stored
                            "clearing_results": {"total_clearings": summary_data.get("clearing_features_count", 0)},
                            "earthwork_results": {
                                "mound_clusters_count": summary_data.get("earthwork_mounds_count", 0),
                                "linear_features_count": summary_data.get("earthwork_linear_count", 0)
                                }
                        }
                        return loaded_results
                    else:
                        logger.warning(f"CACHE INCOMPLETE: Summary found but GeoJSON files missing for {zone_id}/{scene_id}. Re-processing.")
            except Exception as e:
                logger.error(f"Error loading GEDI detection summary from cache: {e}. Re-processing.", exc_info=True)

        logger.info(f"CACHE MISS: Running GEDI detection for {zone_id}/{scene_id}")
        try:
            # Load GEDI metrics - support both JSON (new L2A/L2B) and .npy (legacy) formats
            metrics_data = self._load_gedi_metrics(scene_metrics_path)
            if metrics_data is None:
                logger.error(f"Failed to load GEDI metrics from {scene_metrics_path}")
                return {"success": False, "status": "error_missing_metric_files", "message": "Missing or invalid GEDI metric files"}

            coordinates = metrics_data['coordinates']
            rh95_data = metrics_data['rh95_data']
            rh100_data = metrics_data['rh100_data']
            elevation_data = metrics_data['elevation_data']

            if coordinates.size == 0:
                 logger.warning(f"No coordinate data found in {coords_file}. Skipping GEDI detection for {scene_id}.")
                 return {"success": True, "status": "no_data_points", "total_features": 0, "clearing_results": {}, "earthwork_results": {}}


            logger.info(f"Loaded GEDI metrics for {scene_id}: {coordinates.shape[0]} points")

            # Extract zone-specific parameters for GEDI detection
            zone_thresholds = self.zone_config['thresholds']
            zone_weights = self.zone_config['detection_weights']
            
            # Zone-specific clustering parameters
            if hasattr(self.zone, 'zone_type') and self.zone.zone_type == "deforested_visible_earthworks":
                # Acre-style parameters: larger minimum cluster sizes, wider clustering distances
                min_cluster_size = 5  # Larger earthworks require more points
                clustering_eps = 0.003  # Wider clustering for spread-out earthworks
                logger.info("ðŸŽ¯ Using deforested earthworks GEDI parameters")
            else:
                # Forested buried sites: tighter clustering, smaller features
                min_cluster_size = 3  # Detect smaller buried features
                clustering_eps = 0.002  # Tighter clustering for settlement features
                logger.info("ðŸŽ¯ Using forested buried sites GEDI parameters")
            
            logger.info(f"ðŸŽ¯ Zone-specific GEDI clustering: eps={clustering_eps}, min_size={min_cluster_size}")

            clearing_results = detect_archaeological_clearings(
                rh95_data, rh100_data, coordinates, 
                min_cluster_size=min_cluster_size, 
                clustering_eps=clustering_eps
            )
            earthwork_results = detect_archaeological_earthworks(
                elevation_data, coordinates,
                min_cluster_size=min_cluster_size, 
                clustering_eps=clustering_eps
            )

            total_clearings = clearing_results.get("total_clearings", 0)
            total_earthwork_mounds = len(earthwork_results.get("mound_clusters", []))
            total_earthwork_linear = len(earthwork_results.get("linear_features", []))
            total_features = total_clearings + total_earthwork_mounds + total_earthwork_linear

            # Save results to cache
            files_saved_successfully = True
            
            # Save clearings with proper metadata and LiDAR analysis fields
            gap_clusters_with_metadata = []
            for cluster in clearing_results.get("gap_clusters", []):
                cluster_with_meta = cluster.copy()
                # Get current GEDI parameters for confidence
                params = get_current_params()
                gedi_params = params['gedi']
                
                # Calculate analysis fields for export manager
                cluster_center = cluster.get("center", [0, 0])
                cluster_count = cluster.get("count", 1)
                area_km2 = cluster.get("area_km2", 0)
                
                # Calculate basic elevation statistics for this cluster region
                # Find elevation data near cluster center (approximate)
                if len(coordinates) > 0 and len(elevation_data) > 0:
                    center_lat, center_lon = cluster_center
                    # Find points within cluster region (rough approximation)
                    distances = np.sqrt((coordinates[:, 1] - center_lat)**2 + (coordinates[:, 0] - center_lon)**2)
                    cluster_radius = np.sqrt(area_km2 / np.pi) * 111.32  # Convert km to degrees roughly
                    nearby_mask = distances <= max(cluster_radius, 0.001)  # At least 100m radius
                    
                    if np.any(nearby_mask):
                        nearby_elevations = elevation_data[nearby_mask]
                        valid_elevations = nearby_elevations[~np.isnan(nearby_elevations)]
                        
                        if len(valid_elevations) > 0:
                            mean_elevation = float(np.mean(valid_elevations))
                            elevation_std = float(np.std(valid_elevations))
                            local_variance = float(np.var(valid_elevations))
                        else:
                            mean_elevation = elevation_std = local_variance = None
                    else:
                        mean_elevation = elevation_std = local_variance = None
                else:
                    mean_elevation = elevation_std = local_variance = None
                
                cluster_with_meta.update({
                    "provider": "gedi",
                    "confidence": gedi_params.clearing_confidence,  # Configurable confidence
                    "type": "gedi_clearing",
                    "area_m2": area_km2 * 1000000,  # Convert km2 to m2
                    "coordinates": list(cluster_center),  # Add coordinates field for compatibility
                    
                    # LiDAR analysis fields expected by export manager
                    "gap_points_detected": cluster_count,
                    "mean_elevation": mean_elevation,
                    "elevation_std": elevation_std,
                    "elevation_anomaly_threshold": earthwork_results.get("elevation_stats", {}).get("anomaly_threshold", None),
                    "local_variance": local_variance,
                    "pulse_density": cluster_count / max(area_km2 * 1000000, 490.87) if area_km2 > 0 else None  # Points per m2
                })
                gap_clusters_with_metadata.append(cluster_with_meta)
            
            clearing_gdf = _create_geojson_from_features(gap_clusters_with_metadata, "clearings", "center", 
                ["count", "area_km2", "provider", "confidence", "type", "area_m2", "gap_points_detected", 
                 "mean_elevation", "elevation_std", "elevation_anomaly_threshold", "local_variance", "pulse_density"])
            if clearing_gdf is not None and not clearing_gdf.empty:
                try:
                    clearing_gdf.to_file(clearings_geojson_path, driver="GeoJSON")
                    logger.info(f"Saved GEDI clearings to {clearings_geojson_path}")
                except Exception as e:
                    logger.error(f"Failed to save GEDI clearings GeoJSON: {e}", exc_info=True)
                    files_saved_successfully = False
            elif clearing_results.get("gap_clusters"): # If there were clusters but GDF failed or was empty
                 logger.warning("Clearing clusters detected but GeoDataFrame was empty or failed to generate.")
            
            # Save earthworks (mounds and linear features separately or combined)
            # For simplicity, combining into one GeoJSON for "earthworks" but could be separate
            earthwork_features_for_gdf = []
            mound_clusters = earthwork_results.get("mound_clusters", [])
            for mc in mound_clusters: # mound_clusters are lists of dicts
                mc_copy = mc.copy()
                mc_copy["feature_subtype"] = "mound_cluster"
                
                # Add LiDAR analysis fields for mounds as well
                mound_center = mc.get("center", [0, 0])
                mound_count = mc.get("count", 1)
                mound_area_km2 = mc.get("area_km2", 0)
                
                # Calculate elevation statistics for mound
                if len(coordinates) > 0 and len(elevation_data) > 0:
                    center_lat, center_lon = mound_center
                    distances = np.sqrt((coordinates[:, 1] - center_lat)**2 + (coordinates[:, 0] - center_lon)**2)
                    mound_radius = np.sqrt(mound_area_km2 / np.pi) * 111.32
                    nearby_mask = distances <= max(mound_radius, 0.001)
                    
                    if np.any(nearby_mask):
                        nearby_elevations = elevation_data[nearby_mask]
                        valid_elevations = nearby_elevations[~np.isnan(nearby_elevations)]
                        
                        if len(valid_elevations) > 0:
                            mound_mean_elevation = float(np.mean(valid_elevations))
                            mound_elevation_std = float(np.std(valid_elevations))
                            mound_local_variance = float(np.var(valid_elevations))
                        else:
                            mound_mean_elevation = mound_elevation_std = mound_local_variance = None
                    else:
                        mound_mean_elevation = mound_elevation_std = mound_local_variance = None
                else:
                    mound_mean_elevation = mound_elevation_std = mound_local_variance = None
                
                mc_copy.update({
                    "provider": "gedi",
                    "type": "gedi_mound",
                    "area_m2": mound_area_km2 * 1000000,
                    "coordinates": list(mound_center),
                    
                    # LiDAR analysis fields
                    "gap_points_detected": mound_count,
                    "mean_elevation": mound_mean_elevation,
                    "elevation_std": mound_elevation_std,
                    "elevation_anomaly_threshold": earthwork_results.get("elevation_stats", {}).get("anomaly_threshold", None),
                    "local_variance": mound_local_variance,
                    "pulse_density": mound_count / max(mound_area_km2 * 1000000, 490.87) if mound_area_km2 > 0 else None
                })
                
                earthwork_features_for_gdf.append(mc_copy)

            linear_feats = earthwork_results.get("linear_features", []) # linear_features are lists of dicts
            for lf in linear_feats:
                lf_copy = lf.copy()
                lf_copy["feature_subtype"] = "linear_feature"
                
                # Add LiDAR analysis fields for linear features
                linear_coords = lf.get("coordinates", [])
                if linear_coords and len(linear_coords) > 0:
                    # Calculate centroid for analysis
                    linear_lats = [coord[0] for coord in linear_coords if len(coord) >= 2]
                    linear_lons = [coord[1] for coord in linear_coords if len(coord) >= 2]
                    if linear_lats and linear_lons:
                        centroid_lat = np.mean(linear_lats)
                        centroid_lon = np.mean(linear_lons)
                        
                        # Find elevation data along the linear feature
                        if len(coordinates) > 0 and len(elevation_data) > 0:
                            # Use broader search radius for linear features
                            distances = np.sqrt((coordinates[:, 1] - centroid_lat)**2 + (coordinates[:, 0] - centroid_lon)**2)
                            nearby_mask = distances <= 0.002  # ~200m radius
                            
                            if np.any(nearby_mask):
                                nearby_elevations = elevation_data[nearby_mask]
                                valid_elevations = nearby_elevations[~np.isnan(nearby_elevations)]
                                
                                if len(valid_elevations) > 0:
                                    linear_mean_elevation = float(np.mean(valid_elevations))
                                    linear_elevation_std = float(np.std(valid_elevations))
                                    linear_local_variance = float(np.var(valid_elevations))
                                    linear_point_count = len(valid_elevations)
                                else:
                                    linear_mean_elevation = linear_elevation_std = linear_local_variance = None
                                    linear_point_count = 0
                            else:
                                linear_mean_elevation = linear_elevation_std = linear_local_variance = None
                                linear_point_count = 0
                        else:
                            linear_mean_elevation = linear_elevation_std = linear_local_variance = None
                            linear_point_count = 0
                    else:
                        linear_mean_elevation = linear_elevation_std = linear_local_variance = None
                        linear_point_count = 0
                else:
                    linear_mean_elevation = linear_elevation_std = linear_local_variance = None
                    linear_point_count = 0
                
                lf_copy.update({
                    "provider": "gedi",
                    "type": "gedi_linear",
                    
                    # LiDAR analysis fields
                    "gap_points_detected": linear_point_count,
                    "mean_elevation": linear_mean_elevation,
                    "elevation_std": linear_elevation_std,
                    "elevation_anomaly_threshold": earthwork_results.get("elevation_stats", {}).get("anomaly_threshold", None),
                    "local_variance": linear_local_variance,
                    "pulse_density": None  # Not applicable to linear features
                })
                
                # geom_key for linear features is 'coordinates' (list of points)
                earthwork_features_for_gdf.append(lf_copy)

            # Need a way to tell _create_geojson_from_features about different geom_keys per subtype
            # Simpler: save mounds and linear features separately if their geometry representation differs significantly
            
            # Save Mounds with enhanced properties
            mound_gdf = _create_geojson_from_features(mound_clusters, "earthworks_mounds", "center", 
                ["count", "area_km2", "provider", "type", "area_m2", "gap_points_detected", 
                 "mean_elevation", "elevation_std", "elevation_anomaly_threshold", "local_variance", "pulse_density"])
            # Save Linear Features (geom_key is 'coordinates') with enhanced properties
            linear_gdf = _create_geojson_from_features(linear_feats, "earthworks_linear", "coordinates", 
                ["r2", "length_km", "provider", "type", "gap_points_detected", 
                 "mean_elevation", "elevation_std", "elevation_anomaly_threshold", "local_variance", "pulse_density"])

            # Combine GDFs if both exist, or use whichever one exists
            combined_earthworks_gdf = None
            if mound_gdf is not None and linear_gdf is not None:
                # Note: pd.concat might fail if schemas differ too much beyond geometry.
                # Ensure consistent property names or handle merging carefully.
                # For now, let's try a simple concat. May need more robust merging.
                try:
                    combined_earthworks_gdf = gpd.pd.concat([mound_gdf, linear_gdf], ignore_index=True)
                except Exception as e_concat:
                     logger.warning(f"Could not concat mound and linear GDFs: {e_concat}. Saving separately if possible or only one if other failed.")
                     if mound_gdf is not None and not mound_gdf.empty: combined_earthworks_gdf = mound_gdf
                     elif linear_gdf is not None and not linear_gdf.empty: combined_earthworks_gdf = linear_gdf

            elif mound_gdf is not None:
                combined_earthworks_gdf = mound_gdf
            elif linear_gdf is not None:
                combined_earthworks_gdf = linear_gdf

            if combined_earthworks_gdf is not None and not combined_earthworks_gdf.empty:
                try:
                    combined_earthworks_gdf.to_file(earthworks_geojson_path, driver="GeoJSON")
                    logger.info(f"Saved GEDI earthworks to {earthworks_geojson_path}")
                except Exception as e:
                    logger.error(f"Failed to save GEDI earthworks GeoJSON: {e}", exc_info=True)
                    files_saved_successfully = False
            elif mound_clusters or linear_feats: # If features existed but GDF failed
                 logger.warning("Earthwork features detected but GeoDataFrame was empty or failed to generate.")


            # Create and save summary JSON
            current_results_summary = {
                "status": "success_cached" if files_saved_successfully else "success_partially_cached",
                "zone_id": zone_id,
                "scene_id": scene_id,
                "scene_metrics_path": str(scene_metrics_path),
                "processing_timestamp": datetime.now().isoformat(),
                "clearing_features_count": total_clearings,
                "earthwork_mounds_count": total_earthwork_mounds,
                "earthwork_linear_count": total_earthwork_linear,
                "total_features": total_features,
                "clearing_geojson_path": str(clearings_geojson_path) if (total_clearings > 0 and clearings_geojson_path.exists()) else None,
                "earthworks_geojson_path": str(earthworks_geojson_path) if ((total_earthwork_mounds + total_earthwork_linear) > 0 and earthworks_geojson_path.exists()) else None,
                "notes": "Successfully processed and cached." if files_saved_successfully else "Processed, but some cache files may have failed to save."
            }
            try:
                with open(summary_path, 'w') as f:
                    json.dump(current_results_summary, f, indent=2)
                logger.info(f"Saved GEDI detection summary to {summary_path}")
            except Exception as e:
                logger.error(f"Failed to save GEDI detection summary JSON: {e}", exc_info=True)


            self.detection_results = {
                "success": True,
                "status": "processed_new", # Indicate it was a fresh run
                "zone_name": self.zone.name,
                "scene_path": str(scene_metrics_path), # This is the input metrics path
                "clearing_results": clearing_results,
                "earthwork_results": earthwork_results,
                "total_features": total_features,
                "output_summary_path": str(summary_path), # Path to the cache summary
            }
            return self.detection_results

        except FileNotFoundError as e:
            logger.error(f"Metric file not found during GEDI detection for {scene_id}: {e}", exc_info=True)
            return {"success": False, "status": "error_metric_file_not_found", "message": str(e)}
        except Exception as e:
            logger.error(f"Error during GEDI detection for {scene_id}: {e}", exc_info=True)
            return {"success": False, "status": "error_exception_in_detection", "message": str(e)}

