{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Archaeological Discovery - Analysis Notebook\n",
    "\n",
    "Interactive analysis and exploration of archaeological discoveries in the Amazon Basin.\n",
    "\n",
    "## Overview\n",
    "This notebook provides interactive tools for:\n",
    "- Exploring detected archaeological features\n",
    "- Analyzing convergent anomaly scores\n",
    "- Visualizing results on interactive maps\n",
    "- Generating detailed reports\n",
    "- Validating detection algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Analysis of Detection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze detection method effectiveness\n",
    "if zone_details:\n",
    "    print(\"\\nðŸ“ˆ DETECTION METHOD ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Extract evidence types across all zones\n",
    "    evidence_analysis = {}\n",
    "    \n",
    "    for zone_id, details in zone_details.items():\n",
    "        evidence_list = details.get('evidence_summary', [])\n",
    "        \n",
    "        for evidence in evidence_list:\n",
    "            if evidence not in evidence_analysis:\n",
    "                evidence_analysis[evidence] = {'count': 0, 'zones': [], 'avg_score': 0}\n",
    "            \n",
    "            evidence_analysis[evidence]['count'] += 1\n",
    "            evidence_analysis[evidence]['zones'].append(zone_id)\n",
    "    \n",
    "    # Calculate effectiveness metrics\n",
    "    for evidence_type, data in evidence_analysis.items():\n",
    "        scores = [zone_details[zone]['convergent_score'] for zone in data['zones']]\n",
    "        data['avg_score'] = np.mean(scores) if scores else 0\n",
    "        data['max_score'] = np.max(scores) if scores else 0\n",
    "    \n",
    "    # Create effectiveness DataFrame\n",
    "    eff_df = pd.DataFrame([\n",
    "        {\n",
    "            'Evidence_Type': etype,\n",
    "            'Frequency': data['count'],\n",
    "            'Avg_Score': data['avg_score'],\n",
    "            'Max_Score': data['max_score'],\n",
    "            'Effectiveness': data['avg_score'] / data['count'] if data['count'] > 0 else 0\n",
    "        }\n",
    "        for etype, data in evidence_analysis.items()\n",
    "    ]).sort_values('Effectiveness', ascending=False)\n",
    "    \n",
    "    print(\"Evidence Type Effectiveness:\")\n",
    "    display(eff_df)\n",
    "    \n",
    "    # Visualize evidence effectiveness\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Evidence frequency\n",
    "    ax1.bar(range(len(eff_df)), eff_df['Frequency'], alpha=0.7)\n",
    "    ax1.set_xlabel('Evidence Type')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Evidence Type Frequency')\n",
    "    ax1.set_xticks(range(len(eff_df)))\n",
    "    ax1.set_xticklabels([e[:20] + '...' if len(e) > 20 else e for e in eff_df['Evidence_Type']], \n",
    "                        rotation=45, ha='right')\n",
    "    \n",
    "    # Effectiveness scatter\n",
    "    scatter = ax2.scatter(eff_df['Frequency'], eff_df['Avg_Score'], \n",
    "                         s=eff_df['Max_Score']*10, alpha=0.6, c=eff_df['Effectiveness'], \n",
    "                         cmap='viridis')\n",
    "    ax2.set_xlabel('Frequency')\n",
    "    ax2.set_ylabel('Average Score')\n",
    "    ax2.set_title('Evidence Frequency vs Average Score\\n(size = max score, color = effectiveness)')\n",
    "    plt.colorbar(scatter, ax=ax2, label='Effectiveness')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No data available for statistical analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Recommendations for Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if report_data:\n",
    "    recommendations = report_data.get('recommendations', {})\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ ACTIONABLE RECOMMENDATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Immediate actions\n",
    "    immediate = recommendations.get('immediate_actions', [])\n",
    "    if immediate:\n",
    "        print(\"\\nðŸš¨ IMMEDIATE ACTIONS:\")\n",
    "        for i, action in enumerate(immediate, 1):\n",
    "            print(f\"  {i}. {action}\")\n",
    "    \n",
    "    # Follow-up analysis\n",
    "    followup = recommendations.get('follow_up_analysis', [])\n",
    "    if followup:\n",
    "        print(\"\\nðŸ” FOLLOW-UP ANALYSIS:\")\n",
    "        for i, action in enumerate(followup, 1):\n",
    "            print(f\"  {i}. {action}\")\n",
    "    \n",
    "    # Resource allocation\n",
    "    resources = recommendations.get('resource_allocation', {})\n",
    "    if resources:\n",
    "        print(\"\\nðŸ’° RESOURCE ALLOCATION:\")\n",
    "        for key, value in resources.items():\n",
    "            if isinstance(value, list):\n",
    "                print(f\"  {key.replace('_', ' ').title()}:\")\n",
    "                for item in value:\n",
    "                    print(f\"    - {item}\")\n",
    "            else:\n",
    "                print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "    \n",
    "    # Methodology improvements\n",
    "    methods = recommendations.get('methodology_improvements', [])\n",
    "    if methods:\n",
    "        print(\"\\nðŸ”¬ METHODOLOGY IMPROVEMENTS:\")\n",
    "        for i, improvement in enumerate(methods, 1):\n",
    "            print(f\"  {i}. {improvement}\")\n",
    "            \n",
    "else:\n",
    "    print(\"No recommendations available. Run the full pipeline to generate recommendations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Custom Analysis Functions\n",
    "\n",
    "Interactive tools for custom analysis and exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_zone_in_detail(zone_id):\n",
    "    \"\"\"Detailed analysis of a specific zone\"\"\"\n",
    "    \n",
    "    if zone_id not in TARGET_ZONES:\n",
    "        print(f\"Zone '{zone_id}' not found. Available zones: {list(TARGET_ZONES.keys())}\")\n",
    "        return\n",
    "    \n",
    "    zone = TARGET_ZONES[zone_id]\n",
    "    details = zone_details.get(zone_id, {})\n",
    "    \n",
    "    print(f\"\\nðŸ›ï¸ DETAILED ANALYSIS: {zone.name.upper()}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"Coordinates: {zone.center[0]:.4f}Â°, {zone.center[1]:.4f}Â°\")\n",
    "    print(f\"Priority: {zone.priority} {'â­' * (4 - zone.priority)}\")\n",
    "    print(f\"Search Radius: {zone.search_radius_km} km\")\n",
    "    print(f\"Expected Features: {zone.expected_features}\")\n",
    "    print(f\"Historical Evidence: {zone.historical_evidence}\")\n",
    "    \n",
    "    if details:\n",
    "        print(f\"\\nANALYSIS RESULTS:\")\n",
    "        print(f\"Convergent Anomaly Score: {details['convergent_score']}/15\")\n",
    "        print(f\"Classification: {details['classification']}\")\n",
    "        print(f\"Recommended Action: {details['recommended_action']}\")\n",
    "        \n",
    "        analysis = details['analysis_summary']\n",
    "        print(f\"\\nFEATURE DETECTIONS:\")\n",
    "        print(f\"Terra Preta Patches: {analysis['terra_preta_patches']}\")\n",
    "        print(f\"Geometric Features: {analysis['geometric_features']}\")\n",
    "        print(f\"Total Detections: {analysis['total_detections']}\")\n",
    "        print(f\"Scenes Analyzed: {analysis['scenes_analyzed']}\")\n",
    "        \n",
    "        evidence = details.get('evidence_summary', [])\n",
    "        if evidence:\n",
    "            print(f\"\\nEVIDENCE SUMMARY:\")\n",
    "            for i, ev in enumerate(evidence, 1):\n",
    "                print(f\"  {i}. {ev}\")\n",
    "    else:\n",
    "        print(\"\\nâŒ No analysis results available for this zone.\")\n",
    "        print(\"Run the pipeline with this zone included to generate results.\")\n",
    "\n",
    "def compare_zones(zone_ids):\n",
    "    \"\"\"Compare multiple zones side by side\"\"\"\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for zone_id in zone_ids:\n",
    "        if zone_id not in TARGET_ZONES:\n",
    "            print(f\"Zone '{zone_id}' not found. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        zone = TARGET_ZONES[zone_id]\n",
    "        details = zone_details.get(zone_id, {})\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Zone': zone.name,\n",
    "            'Priority': zone.priority,\n",
    "            'Score': details.get('convergent_score', 0),\n",
    "            'Classification': details.get('classification', 'Not analyzed'),\n",
    "            'Features': details.get('analysis_summary', {}).get('total_detections', 0),\n",
    "            'Coordinates': f\"{zone.center[0]:.4f}, {zone.center[1]:.4f}\"\n",
    "        })\n",
    "    \n",
    "    if comparison_data:\n",
    "        comp_df = pd.DataFrame(comparison_data)\n",
    "        display(comp_df)\n",
    "        \n",
    "        # Quick visualization\n",
    "        if len(comparison_data) > 1:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "            \n",
    "            ax1.bar(comp_df['Zone'], comp_df['Score'])\n",
    "            ax1.set_title('Anomaly Scores Comparison')\n",
    "            ax1.set_ylabel('Score')\n",
    "            ax1.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            ax2.bar(comp_df['Zone'], comp_df['Features'])\n",
    "            ax2.set_title('Features Detected Comparison')\n",
    "            ax2.set_ylabel('Number of Features')\n",
    "            ax2.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "    else:\n",
    "        print(\"No valid zones provided for comparison.\")\n",
    "\n",
    "def suggest_next_zones():\n",
    "    \"\"\"Suggest which zones to analyze next based on priority and current results\"\"\"\n",
    "    \n",
    "    analyzed_zones = set(zone_details.keys())\n",
    "    all_zones = set(TARGET_ZONES.keys())\n",
    "    unanalyzed_zones = all_zones - analyzed_zones\n",
    "    \n",
    "    if unanalyzed_zones:\n",
    "        print(\"\\nðŸŽ¯ SUGGESTED ZONES FOR NEXT ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Sort by priority\n",
    "        suggestions = sorted(\n",
    "            [(zid, TARGET_ZONES[zid]) for zid in unanalyzed_zones],\n",
    "            key=lambda x: x[1].priority\n",
    "        )\n",
    "        \n",
    "        for i, (zone_id, zone) in enumerate(suggestions, 1):\n",
    "            print(f\"{i}. {zone.name} (Priority {zone.priority})\")\n",
    "            print(f\"   Expected: {zone.expected_features}\")\n",
    "            print(f\"   Evidence: {zone.historical_evidence[:100]}...\")\n",
    "            print()\n",
    "            \n",
    "    else:\n",
    "        print(\"âœ… All configured zones have been analyzed!\")\n",
    "        \n",
    "        # Suggest re-analysis with different parameters\n",
    "        low_scoring = [zid for zid, details in zone_details.items() \n",
    "                      if details.get('convergent_score', 0) < 4]\n",
    "        \n",
    "        if low_scoring:\n",
    "            print(\"\\nðŸ”„ ZONES FOR RE-ANALYSIS (low scores):\")\n",
    "            for zone_id in low_scoring:\n",
    "                zone = TARGET_ZONES[zone_id]\n",
    "                score = zone_details[zone_id]['convergent_score']\n",
    "                print(f\"  - {zone.name} (Score: {score}/15)\")\n",
    "\n",
    "# Usage examples\n",
    "print(\"\\nðŸ”§ CUSTOM ANALYSIS FUNCTIONS LOADED\")\n",
    "print(\"Available functions:\")\n",
    "print(\"  - analyze_zone_in_detail('zone_id')\")\n",
    "print(\"  - compare_zones(['zone1', 'zone2', ...])\")\n",
    "print(\"  - suggest_next_zones()\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  analyze_zone_in_detail('negro_madeira')\")\n",
    "print(\"  compare_zones(['negro_madeira', 'trombetas'])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results\n",
    "\n",
    "Export analysis results in various formats for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results_summary():\n",
    "    \"\"\"Export a comprehensive summary of all results\"\"\"\n",
    "    \n",
    "    if not zone_details:\n",
    "        print(\"No results to export. Run analysis first.\")\n",
    "        return\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    \n",
    "    for zone_id, details in zone_details.items():\n",
    "        zone = TARGET_ZONES[zone_id]\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Zone_ID': zone_id,\n",
    "            'Zone_Name': zone.name,\n",
    "            'Latitude': zone.center[0],\n",
    "            'Longitude': zone.center[1],\n",
    "            'Priority': zone.priority,\n",
    "            'Anomaly_Score': details['convergent_score'],\n",
    "            'Classification': details['classification'],\n",
    "            'Terra_Preta_Patches': details['analysis_summary']['terra_preta_patches'],\n",
    "            'Geometric_Features': details['analysis_summary']['geometric_features'],\n",
    "            'Total_Features': details['analysis_summary']['total_detections'],\n",
    "            'Scenes_Analyzed': details['analysis_summary']['scenes_analyzed'],\n",
    "            'Expected_Features': zone.expected_features,\n",
    "            'Historical_Evidence': zone.historical_evidence,\n",
    "            'Recommended_Action': details['recommended_action']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Export to various formats\n",
    "    timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # CSV export\n",
    "    csv_path = f'archaeological_summary_{timestamp}.csv'\n",
    "    summary_df.to_csv(csv_path, index=False)\n",
    "    print(f\"âœ“ CSV exported: {csv_path}\")\n",
    "    \n",
    "    # Excel export with multiple sheets\n",
    "    excel_path = f'archaeological_analysis_{timestamp}.xlsx'\n",
    "    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "        summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "        \n",
    "        # High priority sites\n",
    "        if high_priority_sites:\n",
    "            priority_df = pd.DataFrame(high_priority_sites)\n",
    "            priority_df.to_excel(writer, sheet_name='High_Priority_Sites', index=False)\n",
    "        \n",
    "        # Executive summary\n",
    "        if executive_summary:\n",
    "            exec_df = pd.DataFrame([executive_summary])\n",
    "            exec_df.to_excel(writer, sheet_name='Executive_Summary', index=False)\n",
    "    \n",
    "    print(f\"âœ“ Excel exported: {excel_path}\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# Run export\n",
    "if zone_details:\n",
    "    print(\"Exporting results...\")\n",
    "    exported_df = export_results_summary()\n",
    "    print(\"\\nðŸ“Š EXPORT PREVIEW:\")\n",
    "    display(exported_df.head())\nelse:\n    print(\"No results available for export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interactive Zone Explorer\n",
    "\n",
    "Use the functions below to interactively explore specific zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive exploration - uncomment and modify as needed\n",
    "\n",
    "# Analyze a specific zone in detail\n",
    "# analyze_zone_in_detail('negro_madeira')\n",
    "\n",
    "# Compare multiple zones\n",
    "# compare_zones(['negro_madeira', 'trombetas', 'upper_xingu'])\n",
    "\n",
    "# Get suggestions for next analysis\n",
    "suggest_next_zones()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ›ï¸ AMAZON ARCHAEOLOGICAL DISCOVERY ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review high-priority sites for ground verification\")\n",
    "print(\"2. Plan field expeditions based on recommendations\")\n",
    "print(\"3. Acquire additional satellite data for promising zones\")\n",
    "print(\"4. Refine detection algorithms based on results\")\n",
    "print(\"5. Expand analysis to additional target zones\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Import pipeline components\n",
    "from src.config import TARGET_ZONES, ScoringConfig\n",
    "from src.scoring import ConvergentAnomalyScorer, batch_score_zones\n",
    "from src.visualizers import ArchaeologicalVisualizer\n",
    "from src.detectors import ArchaeologicalDetector\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")\n",
    "print(f\"Target zones configured: {len(TARGET_ZONES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Analysis Results\n",
    "\n",
    "Load results from the main pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest results\n",
    "results_dir = Path('../results')\n",
    "reports_dir = results_dir / 'reports'\n",
    "\n",
    "# Find most recent report\n",
    "if reports_dir.exists():\n",
    "    report_files = list(reports_dir.glob('archaeological_discovery_report_*.json'))\n",
    "    if report_files:\n",
    "        latest_report = max(report_files, key=lambda x: x.stat().st_mtime)\n",
    "        \n",
    "        import json\n",
    "        with open(latest_report, 'r') as f:\n",
    "            report_data = json.load(f)\n",
    "        \n",
    "        print(f\"âœ“ Loaded report: {latest_report.name}\")\n",
    "        print(f\"Session ID: {report_data['session_info']['session_id']}\")\n",
    "        print(f\"Zones analyzed: {len(report_data['zone_details'])}\")\n",
    "        \n",
    "        # Extract key data\n",
    "        zone_details = report_data['zone_details']\n",
    "        high_priority_sites = report_data['high_priority_sites']\n",
    "        executive_summary = report_data['executive_summary']\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ No report files found. Run the main pipeline first.\")\n",
    "        zone_details = {}\n",
    "        high_priority_sites = []\n",
    "        executive_summary = {}\n",
    "else:\n",
    "    print(\"âŒ Results directory not found. Run the main pipeline first.\")\n",
    "    zone_details = {}\n",
    "    high_priority_sites = []\n",
    "    executive_summary = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Executive Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if executive_summary:\n",
    "    # Create summary metrics display\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Discovery Summary', 'Success Rate', 'Feature Types', 'Top Discovery'),\n",
    "        specs=[[{\"type\": \"indicator\"}, {\"type\": \"indicator\"}],\n",
    "               [{\"type\": \"pie\"}, {\"type\": \"bar\"}]]\n",
    "    )\n",
    "    \n",
    "    # Total zones and discoveries\n",
    "    fig.add_trace(\n",
    "        go.Indicator(\n",
    "            mode=\"number+gauge+delta\",\n",
    "            value=executive_summary.get('high_confidence_sites', 0),\n",
    "            title={\"text\": \"High Confidence Sites\"},\n",
    "            gauge={'axis': {'range': [None, 5]}, 'bar': {'color': \"red\"}}\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Success rate\n",
    "    success_rate = float(executive_summary.get('success_rate', '0%').replace('%', ''))\n",
    "    fig.add_trace(\n",
    "        go.Indicator(\n",
    "            mode=\"gauge+number\",\n",
    "            value=success_rate,\n",
    "            title={'text': \"Success Rate (%)\"},\n",
    "            gauge={'axis': {'range': [None, 100]}, 'bar': {'color': \"green\"}}\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=600, title_text=\"Archaeological Discovery Dashboard\")\n",
    "    fig.show()\n",
    "    \n",
    "    # Print key metrics\n",
    "    print(\"\\nðŸ›ï¸ EXECUTIVE SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Zones Analyzed: {executive_summary.get('total_zones_analyzed', 0)}\")\n",
    "    print(f\"High Confidence Sites: {executive_summary.get('high_confidence_sites', 0)}\")\n",
    "    print(f\"Probable Features: {executive_summary.get('probable_archaeological_features', 0)}\")\n",
    "    print(f\"Total Features: {executive_summary.get('total_features_detected', 0)}\")\n",
    "    print(f\"Success Rate: {executive_summary.get('success_rate', 'N/A')}\")\n",
    "    \n",
    "    if executive_summary.get('highest_scoring_zone'):\n",
    "        best = executive_summary['highest_scoring_zone']\n",
    "        print(f\"\\nðŸ† TOP DISCOVERY: {best['zone_name']}\")\n",
    "        print(f\"Score: {best['score']}/15 points\")\n",
    "        print(f\"Classification: {best['classification']}\")\n",
    "        \n",
    "else:\n",
    "    print(\"No executive summary data available. Run analysis first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Zone-by-Zone Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if zone_details:\n",
    "    # Create zone comparison DataFrame\n",
    "    zone_df = []\n",
    "    \n",
    "    for zone_id, details in zone_details.items():\n",
    "        zone_df.append({\n",
    "            'Zone': details['zone_info']['name'],\n",
    "            'Priority': details['zone_info']['priority'],\n",
    "            'Score': details['convergent_score'],\n",
    "            'Classification': details['classification'],\n",
    "            'Terra_Preta': details['analysis_summary']['terra_preta_patches'],\n",
    "            'Geometric': details['analysis_summary']['geometric_features'],\n",
    "            'Total_Features': details['analysis_summary']['total_detections'],\n",
    "            'Scenes': details['analysis_summary']['scenes_analyzed']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(zone_df)\n",
    "    \n",
    "    # Display zone comparison table\n",
    "    print(\"\\nðŸ“Š ZONE ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    display(df.sort_values('Score', ascending=False))\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Zone Analysis Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Score comparison\n",
    "    bars = axes[0, 0].bar(df['Zone'], df['Score'], \n",
    "                         color=['red' if s >= 10 else 'orange' if s >= 7 else 'yellow' if s >= 4 else 'gray' \n",
    "                               for s in df['Score']])\n",
    "    axes[0, 0].set_title('Convergent Anomaly Scores by Zone')\n",
    "    axes[0, 0].set_ylabel('Score (0-15)')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 0].axhline(y=10, color='red', linestyle='--', alpha=0.7, label='High Confidence')\n",
    "    axes[0, 0].axhline(y=7, color='orange', linestyle='--', alpha=0.7, label='Probable')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Feature distribution\n",
    "    width = 0.35\n",
    "    x = np.arange(len(df))\n",
    "    axes[0, 1].bar(x - width/2, df['Terra_Preta'], width, label='Terra Preta', alpha=0.8)\n",
    "    axes[0, 1].bar(x + width/2, df['Geometric'], width, label='Geometric', alpha=0.8)\n",
    "    axes[0, 1].set_title('Feature Types by Zone')\n",
    "    axes[0, 1].set_ylabel('Number of Features')\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels(df['Zone'], rotation=45)\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Priority vs Performance\n",
    "    scatter = axes[1, 0].scatter(df['Priority'], df['Total_Features'], \n",
    "                                s=df['Score']*20, c=df['Score'], cmap='RdYlBu_r', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Zone Priority')\n",
    "    axes[1, 0].set_ylabel('Total Features Detected')\n",
    "    axes[1, 0].set_title('Priority vs Discovery Success\\n(size & color = score)')\n",
    "    plt.colorbar(scatter, ax=axes[1, 0], label='Anomaly Score')\n",
    "    \n",
    "    # Classification distribution\n",
    "    class_counts = df['Classification'].value_counts()\n",
    "    axes[1, 1].pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%')\n",
    "    axes[1, 1].set_title('Classification Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No zone details available. Run analysis first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. High-Priority Sites for Ground Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if high_priority_sites:\n",
    "    print(\"\\nðŸŽ¯ HIGH-PRIORITY SITES FOR GROUND VERIFICATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, site in enumerate(high_priority_sites, 1):\n",
    "        print(f\"\\n{i}. {site['zone_name'].upper()}\")\n",
    "        print(f\"   Anomaly Score: {site['anomaly_score']}/15\")\n",
    "        print(f\"   Classification: {site['classification']}\")\n",
    "        print(f\"   Coordinates: {site['coordinates'][0]:.4f}Â°, {site['coordinates'][1]:.4f}Â°\")\n",
    "        print(f\"   Expected Features: {site['expected_features']}\")\n",
    "        print(f\"   Access Difficulty: {site['access_difficulty']}\")\n",
    "        print(f\"   Verification Cost: {site['verification_cost_estimate']}\")\n",
    "        print(f\"   Evidence: {', '.join(site['evidence_types'])}\")\n",
    "    \n",
    "    # Create priority visualization\n",
    "    if len(high_priority_sites) > 1:\n",
    "        priority_df = pd.DataFrame(high_priority_sites)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Score ranking\n",
    "        bars = ax1.barh(priority_df['zone_name'], priority_df['anomaly_score'],\n",
    "                       color=['red' if r == 1 else 'orange' for r in priority_df['priority_ranking']])\n",
    "        ax1.set_xlabel('Anomaly Score')\n",
    "        ax1.set_title('High-Priority Sites Ranking')\n",
    "        ax1.axvline(x=10, color='red', linestyle='--', alpha=0.7, label='High Confidence Threshold')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Cost vs Score analysis\n",
    "        # Extract numeric cost estimates\n",
    "        costs = []\n",
    "        for cost_str in priority_df['verification_cost_estimate']:\n",
    "            # Extract first number from cost string\n",
    "            import re\n",
    "            match = re.search(r'\\$(\\d+)', cost_str.replace(',', ''))\n",
    "            if match:\n",
    "                costs.append(int(match.group(1)))\n",
    "            else:\n",
    "                costs.append(50000)  # Default estimate\n",
    "        \n",
    "        scatter = ax2.scatter(priority_df['anomaly_score'], costs, \n",
    "                            s=200, c=priority_df['priority_ranking'], \n",
    "                            cmap='RdYlBu_r', alpha=0.7, edgecolors='black')\n",
    "        ax2.set_xlabel('Anomaly Score')\n",
    "        ax2.set_ylabel('Verification Cost (USD)')\n",
    "        ax2.set_title('Score vs Verification Cost')\n",
    "        \n",
    "        # Add site labels\n",
    "        for i, (score, cost, name) in enumerate(zip(priority_df['anomaly_score'], costs, priority_df['zone_name'])):\n",
    "            ax2.annotate(name.split()[0], (score, cost), xytext=(5, 5), \n",
    "                        textcoords='offset points', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "else:\n",
    "    print(\"No high-priority sites identified. Lower detection thresholds or run more zones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Discovery Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive map of discoveries\n",
    "print(\"Creating interactive discovery map...\")\n",
    "\n",
    "# Check if we have map data\n",
    "maps_dir = results_dir / 'maps'\n",
    "if maps_dir.exists():\n",
    "    map_files = list(maps_dir.glob('archaeological_discoveries_*.html'))\n",
    "    if map_files:\n",
    "        latest_map = max(map_files, key=lambda x: x.stat().st_mtime)\n",
    "        print(f\"âœ“ Latest map available: {latest_map.name}\")\n",
    "        print(f\"Open this file in your browser: {latest_map.absolute()}\")\n",
    "        \n",
    "        # Display map in notebook (if possible)\n",
    "        from IPython.display import IFrame, display\n",
    "        try:\n",
    "            display(IFrame(src=str(latest_map), width=800, height=600))\n",
    "        except:\n",
    "            print(\"Map created but cannot display inline. Open the HTML file directly.\")\n",
    "    else:\n",
    "        print(\"No map files found. Run the visualization step of the pipeline.\")\n",
    "else:\n",
    "    print(\"Maps directory not found. Run the full pipeline first.\")\n",
    "\n",
    "# Alternative: Create a simple map here\n",
    "if zone_details:\n",
    "    print(\"\\nCreating summary map...\")\n",
    "    \n",
    "    # Amazon center\n",
    "    m = folium.Map(location=[-5.0, -60.0], zoom_start=5)\n",
    "    \n",
    "    # Add target zones\n",
    "    for zone_id, details in zone_details.items():\n",
    "        zone = TARGET_ZONES[zone_id]\n",
    "        score = details['convergent_score']\n",
    "        \n",
    "        # Color based on score\n",
    "        if score >= 10:\n",
    "            color = 'red'\n",
    "        elif score >= 7:\n",
    "            color = 'orange'\n",
    "        elif score >= 4:\n",
    "            color = 'yellow'\n",
    "        else:\n",
    "            color = 'blue'\n",
    "        \n",
    "        folium.CircleMarker(\n",
    "            location=[zone.center[0], zone.center[1]],\n",
    "            radius=10 + score,\n",
    "            popup=f\"{zone.name}\\nScore: {score}/15\\n{details['classification']}\",\n",
    "            tooltip=f\"{zone.name} ({score}/15)\",\n",
    "            color='white',\n",
    "            weight=2,\n",
    "            fill=True,\n",
    "            fillColor=color,\n",
    "            fillOpacity=0.8\n",
    "        ).add_to(m)\n",
    "    \n",
    "    # Save and display\n",
    "    summary_map_path = Path('summary_discovery_map.html')\n",
    "    m.save(str(summary_